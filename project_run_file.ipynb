{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_float(dataframe):\n",
    "  '''\n",
    "  INPUT\n",
    "    dataframe - any dataframe acceptable.\n",
    "    col - local variable, used to go through all columns in the dataframe.\n",
    "  OUTPUT\n",
    "    dataframe - global varable. The input dataframe is replaced in the function. If the following \n",
    "    logic is applicable on a column level.  \n",
    "\n",
    "  The function will cycle through each column in the dataframe and replace any dashes or spaces\n",
    "  with the NaN value definition from the numpy library. If the column is numeric in nature but\n",
    "  formatted as an object it will change it's datatype to a float. If the column is naturally a \n",
    "  string then the function will print \"Could not convert\", followed by the column name and move\n",
    "  on to the next column.\n",
    "\n",
    "  While debugging this function and viewing the dataset, at first look I believed all blanks in \n",
    "  the data were denoted by a '-', however one factor - that was a scored factor - was failing \n",
    "  (ef_regulation_labor_dismissal), and the currently commented out section identified the row in \n",
    "  that column that failed and that it was an empty space. \n",
    "\n",
    "  Therefore, if a column that you expect to able to turn into a float fails, then you can \n",
    "  uncomment the green section and rerun the function to get the row number that is failing, and \n",
    "  subsequently find out why it failed. I've kept it commented out because if the whole column\n",
    "  fails for another reason then it will print every row of that column, so it's best used when\n",
    "  a problem column has already been identified.   \n",
    "  '''\n",
    "\n",
    "\n",
    "  for col in dataframe:\n",
    "    try:\n",
    "      dataframe[col] = dataframe[col].replace(['-', ' '], np.NaN)\n",
    "      dataframe[col] = dataframe[col].astype(float)\n",
    "    except:\n",
    "      # uncomment the following three lines to debug a failing column that should be able convert.\n",
    "      # for row in dataframe[col]:\n",
    "      #   if not isinstance(row, float):\n",
    "      #     print(row)\n",
    "      print('Could not convert ' + col)\n",
    "\n",
    "\n",
    "def most_missing_col(dataframe, threshold):\n",
    "  '''\n",
    "  INPUT\n",
    "    dataframe - the processed dataframe, with NaNs.\n",
    "    threshold - the threshold for percenatge of missing values.\n",
    "  OUTPUT\n",
    "    missing_col_list - list of variables that have more missing values than the threshold identified.\n",
    "\n",
    "  The function will create a list of variables from the dataframe that have a greater percentage of\n",
    "  missing values than provided by the threshold input. \n",
    "  '''\n",
    "\n",
    "  missing_col_list = list(dataframe.columns[dataframe.isnull().mean() > threshold])\n",
    "\n",
    "  return missing_col_list\n",
    "\n",
    "\n",
    "def country_per_region(selected_region, response, dataframe):\n",
    "  '''\n",
    "  INPUT\n",
    "    selected_region - any region defined by the dataset\n",
    "    response - which variable you want to take the mean of over the region\n",
    "    dataframe - the dataset that contains Region\n",
    "  OUTPUT\n",
    "    out_data - the aggregated mean of the reponse variable over the selected region.\n",
    "\n",
    "  This function works well to loop over multiple regions, or variables to get the mean\n",
    "  of that variable for each region. It can be called inside a loop. But works fine\n",
    "  on its own.\n",
    "\n",
    "'''\n",
    "\n",
    "  out_data = dataframe[dataframe[\"region\"] == selected_region].groupby([\"countries\"]).mean()[response]\n",
    "\n",
    "  return out_data\n",
    "\n",
    "\n",
    "def plot_save_Region_correlation(var_1, var_2, region):\n",
    "  '''\n",
    "  INPUT\n",
    "    var_1 - an input variable for plotting\n",
    "    var_2 - an input varaible for plotting \n",
    "    region - for a selected region\n",
    "  OUTPUT\n",
    "    saved plot. no global returned variable.\n",
    "\n",
    "  This function produces and saves a graph to compare two variables for a filtered region.\n",
    "\n",
    "  '''\n",
    "  save_name = \"women's_freedom_correlated_with_freedom_score_for_\" + region\n",
    "\n",
    "  plt.figure(figsize = (10,8))\n",
    "  plt.subplots_adjust(bottom = 0.2)\n",
    "\n",
    "  plt.plot(var_1, linestyle = 'solid', color = 'r')\n",
    "  plt.plot(var_2, linestyle = 'solid', color = '#4b0082')\n",
    "  plt.ylabel(\"variable score\")\n",
    "  plt.xlabel(\"country\")\n",
    "  plt.xticks(rotation = 90)\n",
    "  plt.legend((\"Human Freedom Score\", \"Personal woman's score\"), loc = 'lower right')\n",
    "  plt.grid(True)\n",
    "\n",
    "  plt.title(\"The average personal women's freedom score aggregated for the top correlated factors. \\n Region = \" + region)\n",
    "\n",
    "  plt.savefig('./results/' + save_name + '.png')\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def correlations_top_bottom(dataframe, region_select, var, n):\n",
    "  '''\n",
    "  INPUT\n",
    "    dataframe - the proccessed dataframe, must contain region.\n",
    "    region_select - the selected region\n",
    "    var - the response variable to check correlations against\n",
    "    n - number of variables to print\n",
    "  OUTPUT\n",
    "    Print top - print the highest correlated factors\n",
    "    Print bottom - print the highest anti-correlated factors\n",
    "\n",
    "  This function prints out the highest correlated and anti-correlated factors compared with a given variable.\n",
    "  The number of highest factors printed is given by n.\n",
    "\n",
    "  '''\n",
    "\n",
    "  correlations = dataframe[dataframe[\"region\"] == region_select].corr()\n",
    "  sorted_corr = correlations[var].sort_values()\n",
    "\n",
    "  Top = sorted_corr[-n:]\n",
    "  print(Top)\n",
    "  Bottom = sorted_corr[:n]\n",
    "  print(Bottom)\n",
    "\n",
    "\n",
    "def variable_per_region(selected_region, var, response, dataframe):\n",
    "  '''\n",
    "  INPUT\n",
    "    selected_region - region of interest\n",
    "    var - correlated variable of interest\n",
    "    response - the orginal variable it was checked for correlations against. \n",
    "  OUTPUT\n",
    "    out_data - the relationship between the correlated pair for a selected region.\n",
    "\n",
    "  This function gives the mean value of the correlated factor to the response factor for a\n",
    "  given region so that they can be plotted before potential modelling.\n",
    "\n",
    "'''\n",
    "\n",
    "  out_data = dataframe[dataframe[\"region\"] == selected_region].groupby([var]).mean()[response]\n",
    "\n",
    "  return out_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year ISO_code  countries                         region hf_score hf_rank  \\\n",
      "0  2017      ALB    Albania                 Eastern Europe     7.84      38   \n",
      "1  2017      DZA    Algeria     Middle East & North Africa     4.99     155   \n",
      "2  2017      AGO     Angola             Sub-Saharan Africa      5.4     151   \n",
      "3  2017      ARG  Argentina  Latin America & the Caribbean     6.86      77   \n",
      "4  2017      ARM    Armenia        Caucasus & Central Asia     7.42      54   \n",
      "\n",
      "  hf_quartile pf_rol_procedural pf_rol_civil pf_rol_criminal  ...  \\\n",
      "0           1               6.7          4.5             4.7  ...   \n",
      "1           4                 -            -               -  ...   \n",
      "2           4                 -            -               -  ...   \n",
      "3           2               7.1          5.8             4.3  ...   \n",
      "4           2                 -            -               -  ...   \n",
      "\n",
      "  ef_regulation_business_adm ef_regulation_business_bureaucracy  \\\n",
      "0                        6.3                                6.7   \n",
      "1                        3.7                                1.8   \n",
      "2                        2.4                                1.3   \n",
      "3                        2.5                                7.1   \n",
      "4                        4.6                                6.2   \n",
      "\n",
      "  ef_regulation_business_start ef_regulation_business_bribes  \\\n",
      "0                          9.7                           4.1   \n",
      "1                          9.3                           3.8   \n",
      "2                          8.7                           1.9   \n",
      "3                          9.6                           3.3   \n",
      "4                          9.9                           4.6   \n",
      "\n",
      "  ef_regulation_business_licensing ef_regulation_business_compliance  \\\n",
      "0                                6                               7.2   \n",
      "1                              8.7                                 7   \n",
      "2                              8.1                               6.8   \n",
      "3                              5.4                               6.5   \n",
      "4                              9.3                               7.1   \n",
      "\n",
      "  ef_regulation_business ef_regulation ef_score ef_rank  \n",
      "0                    6.7           7.8     7.67      30  \n",
      "1                    5.7           5.4     4.77     159  \n",
      "2                    4.9           5.7     4.83     158  \n",
      "3                    5.7           5.6     5.67     147  \n",
      "4                    6.9           7.5      7.7      27  \n",
      "\n",
      "[5 rows x 120 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RUN file for Udacity Data science nanodegree: Project One - Write a Data Science Blog Post\n",
    "Author: Hannah Costa\n",
    "\n",
    "The data chosen for this assigmnet was the Human Freedom Index data avaialble on Kaggle.\n",
    "LINK: https://www.kaggle.com/gsutters/the-human-freedom-index\n",
    "\n",
    "The Human Index data combines measures of economic and personal freedom to assign a total \n",
    "Human Freedom score to a country and rank it based on this score. The survey has been \n",
    "conducted since 2008 for the earliest definition, and the most recent data availiable is \n",
    "from 2019, which is used in this assignment. \n",
    "\n",
    "The data available isn't the raw data from the Freedom project, and is made up of the economic \n",
    "and personal freedom factors that are all on a scored scale between 0-10. The Human Freedom \n",
    "report offers some insight in the meanings behind these factors but their definitions, \n",
    "or what the score directly translates to numerically is unknown.\n",
    "\n",
    "For example the pf_ss_women_fgm factor relates to Female Genital Mutilations in a country, but \n",
    "the score doesn't tell me if it's based on number of cases reported per population percentage.\n",
    "'pf' in a factor means that it's used to create a Personal Freedom Factor and 'ss' means it's \n",
    "in the 'Security and Safety' category. \n",
    "\n",
    "The freedom indicators covered by the report are:\n",
    "\n",
    "Rule of Law\n",
    "Security and Safety\n",
    "Movement\n",
    "Religion\n",
    "Association, Assembly, and Civil Society\n",
    "Expression and Information\n",
    "Identity and Relationships\n",
    "Size of Government\n",
    "Legal System and Property Rights\n",
    "Access to Sound Money\n",
    "Freedom to Trade Internationally\n",
    "Regulation of Credit, Labor, and Business\n",
    "\n",
    "This Run file will call functions and scripts with the purpose of reading in the data and \n",
    "processing it so it can be used to explore correlations and insights in the data while\n",
    "focusing on the implications of women's freedom. The author is using python 3.8.5 in Jupyter notebook.\n",
    "\n",
    "'''\n",
    "# Call the libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Read dataset into a dataframe\n",
    "df = pd.read_csv('./data/datasets_93172_883723_hfi_cc_2019.csv')\n",
    "# Check the first handful of rows to ensure the data was retrieved. note: 120 columns expected\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the processed nature of the data set I want to drop some of the factors that\n",
    "# are generated by the more base factors.\n",
    "df = df.drop([\"ISO_code\", \"ef_score\",\"pf_score\", \"pf_rank\", \"ef_rank\", \"hf_rank\", \"hf_quartile\"], axis = 1)\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert countries\n",
      "Could not convert region\n"
     ]
    }
   ],
   "source": [
    "# The whole dataset comes in as \"objects\" despite the majority of the data being a numerical score. \n",
    "# The next step would be to convert the columns that can be converted to floats in order to be able\n",
    "# to perform numerical analysis on the dataset. \n",
    "\n",
    "convert_float(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pf_rol_procedural', 'pf_rol_civil', 'pf_rol_criminal', 'pf_ss_women_inheritance_widows', 'pf_ss_women_inheritance_daughters', 'pf_religion_estop_establish', 'pf_religion_estop_operate', 'pf_association_political_establish', 'pf_association_political_operate', 'pf_association_prof_establish', 'pf_association_prof_operate', 'pf_association_sport_establish', 'pf_association_sport_operate', 'pf_identity_legal']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a list of columns that have more than 25% of their values missing.\n",
    "col_list = most_missing_col(df, 0.25)\n",
    "\n",
    "# Check column list to be aware of what's being dropped\n",
    "print(col_list)\n",
    "\n",
    "# Drop the columns that have the most missing values\n",
    "df = df.drop(col_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Region_list = list(set(df[\"region\"]))\n",
    "\n",
    "# Check that region_list has populated correctly\n",
    "# print(Region_list)\n",
    "\n",
    "# Loop over region_list for average scores for hf_score and pf_ss_women score. Plot togther for \n",
    "# each country per region. Save the figuers so they do not overwrite each other. \n",
    "\n",
    "for region in Region_list:\n",
    "  region_pf_women = country_per_region(region, \"pf_ss_women\", df)\n",
    "  region_hf_score = country_per_region(region, \"hf_score\", df)\n",
    "  plot_save_Region_correlation(region_hf_score, region_pf_women, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ef_legal                         0.271014\n",
      "ef_legal_protection              0.303193\n",
      "ef_regulation_business_bribes    0.305410\n",
      "ef_legal_military                0.309798\n",
      "ef_legal_judicial                0.360162\n",
      "hf_score                         0.380304\n",
      "pf_ss                            0.637599\n",
      "pf_ss_women_inheritance          0.759284\n",
      "pf_ss_women_fgm                  0.802604\n",
      "pf_ss_women                      1.000000\n",
      "Name: pf_ss_women, dtype: float64\n",
      "ef_government_transfers       -0.349969\n",
      "ef_trade_tariffs_sd           -0.246652\n",
      "ef_government_consumption     -0.231973\n",
      "ef_regulation_labor_firing    -0.200293\n",
      "ef_legal_integrity            -0.194131\n",
      "ef_regulation_labor_bargain   -0.151508\n",
      "pf_association_assembly       -0.114447\n",
      "pf_association_political      -0.099305\n",
      "ef_legal_enforcement          -0.090885\n",
      "ef_government_soa             -0.074465\n",
      "Name: pf_ss_women, dtype: float64\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(correlations_top_bottom(df, \"Sub-Saharan Africa\", \"pf_ss_women\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Closer look at the correlation for Sub Saharan Africa\n",
    "var_list = [\"ef_legal_judicial\", \"ef_legal_military\", \"ef_regulation_business_bribes\", \"ef_government_transfers\", \"ef_trade_tariffs_sd\", \"ef_government_consumption\"]\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel(\"variable score\")\n",
    "plt.ylabel(\"mean pf_ss_women score\")\n",
    "plt.title(\"The average personal women's freedom score aggregated for the top correlated factors. \\n Sub-Saharan Africa\")\n",
    "plt.grid(True)\n",
    "\n",
    "d = {}\n",
    "for var in var_list:\n",
    "  d[var] = variable_per_region(\"Sub-Saharan Africa\", var, \"pf_ss_women\", df)\n",
    "  plt.plot(d[var], 'o')\n",
    "\n",
    "plt.legend((var_list), loc ='lower right')\n",
    "plt.savefig('./results/scatter_correlations_SSA.png')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ef_government_transfers worked as expected\n",
      "The Model performed with a Score of 0.5401751775708902\n"
     ]
    }
   ],
   "source": [
    "#  Can you model womens's freedomn using only economic factors per region.\n",
    "df_modelling = df.dropna(subset = [\"pf_ss_women\"], axis = 0)\n",
    "\n",
    "X = df_modelling.filter(regex = '^ef', axis = 1)\n",
    "y = df_modelling[\"pf_ss_women\"]\n",
    "\n",
    "fill_mean = lambda col: col.fillna(col.mean())\n",
    "\n",
    "# I've kept the following function inside the run_file instead of the function_file to toggle the different print functions on and off when debugging.\n",
    "for col in X:\n",
    "  if X[col].isnull().sum() == 0:\n",
    "    # print(col + \" no NaN's\")\n",
    "    continue\n",
    "  else:\n",
    "    try:\n",
    "      X = X.apply(fill_mean, axis = 0)\n",
    "      print(col + \" worked as expected\")\n",
    "    except:\n",
    "      print(col + \" did not replace NaN with mean\")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 15)\n",
    "lm_model = LinearRegression(normalize = True)\n",
    "lm_model.fit(X_train, y_train)\n",
    "\n",
    "Score = str(lm_model.score(X_test, y_test, sample_weight = None))\n",
    "\n",
    "print(\"The Model performed with a Score of \" + Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
